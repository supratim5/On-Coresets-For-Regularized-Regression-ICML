\section{Introduction}
Most of the applications of machine learning require huge amounts of data to train models. As the algorithms to train models run in time dependent on the size of the data, the task becomes computationally inefficient. A coreset is a subsample of appropriately reweighted points from the original data which can be used to train models with competitive accuracy and provable guarantees. The size of a coreset is usually independent of the the size of the original dataset making training on them much quicker. Sampling based coresets also allow better interpretability in contrast to data reduction techniques which project data points in some other space.

Regression is a widely used technique in machine learning and statistics, the most popular variants being the least square regression ($\ell_2$ regression) and least absolute deviation ($\ell_1$ regression).  A coreset construction for $\ell_2$ regression based on leverage scores is given in \cite{drineas2006sampling} whereas coresets for $\ell_p$ regression have been created based either on norms of the so called well-conditioned basis~\cite{dasgupta2009sampling,sohler2011subspace} or based on Lewis weights~\cite{cohen2015p}.

A common variant of regression is to use regularization, meant to either achieve numerical stability, to prevent overfitting or to induce sparsity in the solution-- ridge and lasso being the most commonly used regularizers. Since regularization imposes a constraint on the solution space, we can potentially expect regularized problems to have a smaller size coreset, than the unregularized version. Indeed, this intuition has been formalized in the case of ridge regression~\cite{woodruff2014sketching} using the ridge leverage scores. Pilanci et al.~\cite{pilanci2015randomized} construct small sized coresets for constrained version of lasso using random projections, these coresets are however not (scaled) subsample of data points. To the best of our knowledge ours is the first work to study coresets for $\ell_p$ regularized regression for $p\neq 2$. Our first result is negative-- we show that 
it is not always possible to build smaller coresets for regularized regression. For a specific class of problems we show that smaller coresets are possible and we show how to construct them.

To construct such coresets, we follow the sensitivity framework given in \cite{feldman2011unified}. Sensitivities defined in~\cite{langberg2010universal} capture the importance of a data point for a particular optimization function. If we sample points using a probability distribution based on sensitivities, the sample size depends on the sum of the sensitivities and the dimension of the solution space. The core idea behind our bounds is that due to regularization, the sensitivities of points change -- while the sensitivity of very low sensitivity points might increase slightly (by additive $1/n$), the higher sensitivities are pulled down. The overall effect is that 
the sum of sensitivities for a regularized version of regression is less than the sum of sensitivities for its unregularized counterpart by a factor that depends on the value of the regularizer. 

\subsection{Our Contributions}
\begin{itemize}
  %$p,q \geq 1$ and $r,s > 0$
	\item We first show that for any regularized problem of the form $\min _{\mathbf{x} \in \mathbb{R}^d} \|\mathbf{Ax}\|_p^r + \lambda \|\M{x}\|_q^s $, where $r \neq s$, coreset for the problem also works as a coreset for its unregularized version. This implies that when $r \neq s$, we cannot build coresets with size smaller than the ones for the unregularized version.
	\item We introduce a modified version of lasso problem for which we show that we can construct coresets with size smaller than that of the unregularized linear regression.
	\item We calculate sensitivity upper bounds for the  $\ell_p $ regression with $\ell_p$ regularization. We focus on $p \geq 1, p\neq 2$. Specifically we give smaller coreset for the regularized least deviation problem i.e. for $p=1$.
	\item We show experimental evidence that the modified lasso problem also preserves sparsity like the lasso problem and hence is a suitable replacement. We also demonstrate the performance of our sampling probabilities for the modified lasso problem and the regularized least deviation problem.
\end{itemize}
\subsection{Organization of the Paper}
The rest of the paper is organized in the following manner. In section 2 we discuss work in the areas related to coresets in general as well as coresets specifically for regularized problems and distinguish our work from existing work. In section 3 we provide all the notations, definitions and existing results that we use throughout the paper. In section 4 we give our main result relating coresets for regularized regression to the ones for unregularized regression. In section 5 we introduce the modified lasso  problem and analyze the size of coreset for it. In section 6 we show smaller coresets for $\ell_p$ regression with $\ell_p$ regularization. In section 7 we report the experiments validating our claims and  conclude in section 8 with discussion on future scope in this area.