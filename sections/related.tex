\section{Related Work}
Coresets are small summaries of data which 
can be used as a proxy to the original data with provable guarantees. The term was first introduced in \cite{agarwal2004approximating} where they used coresets for the shape fitting problem. Coresets for clustering problem were described in~\cite{har2004coresets}. 
In \cite{feldman2011unified} authors gave a generalized framework to construct coresets based on importance sampling using sensitivity scores introduced in \cite{langberg2010universal} which was improved in \cite{braverman2016new} for both offline and streaming settings.

There is a large amount of work to reduce size of the data for the $\ell_2$ regression problem \cite{drineas2011faster,clarkson2017low}. Interested readers may refer to a good survey \cite{woodruff2014sketching} and references therein. Coresets for $\ell_2$ regression can be obtained using the popular leverage scores~\cite{drineas2006sampling}. Work has also been done to obtain these leverage scores in an efficient manner \cite{drineas2012fast}. Coresets for $\ell_p$ regression are obtained using the row norms of the well-conditioned basis~\cite{dasgupta2009sampling} or the Lewis weights~\cite{cohen2015p}. Other works that construct coresets for $\ell_p$ regression in general or $\ell_1$ regression in particular include \cite{sohler2011subspace,meng2013low,woodruff2013subspace,clarkson2016fast,dickens2018leveraging}. 

However not much has been done for regularized version of the regression problem. Pilanci et al.~\cite{pilanci2015randomized} are able to reduce the size of the popular lasso problem using random projections. However they work with the constrained version of the problem and not the regularized one. Also they do not obtain a sample of the original dataset. Reddi et al.~\cite{reddi2015communication} obtained additive error coresets for empirical loss minimization. Coresets using local sensitivity scores \cite{raj2019importance} also work for functions with a regularization term. Here they use a quadratic approximation of the function and then use leverage scores to approximate sensitivities locally. Tolochinsky et al. \cite{tolochinsky2018coresets} actually add a regularization term to obtain coresets for functions for which otherwise sublinear coresets may not exist.

Coresets for logistic regression and SVM with $\ell_2^2$ regularization were obtained by \cite{curtain2019coresets} using uniform sampling. 
They considered empirical loss minimization problems of the form $f(\M{w})=\sum_{i=1}^n l(-y_i{\M{w^T}\M{x_i})}+ \lambda r(R{\M{w}})$ where $\M{x_i}$ is the $i^{th}$ data point with corresponding label $y_i \in \{+1,-1\}$ and ${\M{w}}$ is solution vector. The parameter $R$ is maximum $2$-norm of a row in data matrix.
Their coresets do not work for a general response vector as in regression problems.
In addition, for uniform sampling to work, they assume a $(\sigma,\tau)$ condition on the loss and the regularization functions $l$ and $r$ respectively which says that $l(-\sigma) > 0$, and if $\|\M{w}\|_2 \geq \sigma$
then $r(\M{w}) \geq \tau l(\|\M{w}\|_2) $. Even if we consider response vectors consisting of $\{-1,+1\}$, we have to consider special formulation to apply their result to the specific case. We augment $\M{x'}= \begin{bmatrix} \mathbf{x}\\ y_i \end{bmatrix}$ and $\M{w'}= \begin{bmatrix} \mathbf{w}\\ -1 \end{bmatrix} $. Now say for example we represent lasso problem in their framework for each $y_i \in \{+1,-1\}$ using $\M{x'}$ and $\M{w'}$. For  $l(s) =s^2$ and $r(\M{w})= \|\M{w}\|_1$. Here $l(-\sigma) > 0$ for all values of $\sigma$, however, to satisfy $\|\M{w}\|_1 \geq \tau\|\M{w}\|_2^2$ when $\|\M{w}\|_2 \geq \sigma$, a value of $\tau$ is not there. So their framework does not apply to our case.

In spirit, our work most closely relates to the work in \cite{avron2017sharper}. Here the authors show that for ridge regression, coresets of size smaller than the coreset for the unregularized least square regression problem can be constructed. Their coreset size is a function of the statistical dimension of the matrix for some regularization parameter $\lambda > 0$. It is important to note that a coreset for unregularized regression can be shown to work for regularized regression as well. However the work in \cite{avron2017sharper} supports the intuition that coresets for regularized regression may be smaller than coresets for its unregularized counterpart. We generalize this idea from ridge regression to $\ell_p$ regression with $\ell_p$ regularization in this paper. We also show that for a broad class of regularized regression problems, it is not possible to construct coreset smaller in size compared to its unregularized form.